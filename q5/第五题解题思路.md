# 第五题解题思路 - 能耗优化

## 1. 场景与符号（题目五对齐）

本节面向题目五：在异构网络中，宏基站与微基站协作，需同时进行用户接入决策、资源分配、功率控制和接入模式选择，以在保证用户服务质量（QoS）的同时实现能耗优化。实现基于 `q5` 目录中的环境与深度强化学习（DRL）。

### 1.1 集合与索引
- \( \mathcal{B} = \{\text{MBS}, \text{SBS1}, \text{SBS2}, \text{SBS3}\} \)：基站集合，包含一个宏基站（MBS）和三个微基站（SBS）
- \( \mathcal{S} = \{\text{URLLC}, \text{eMBB}, \text{mMTC}\} \)：切片集合
- \( \mathcal{K}_s \)：切片 \(s\) 的用户集合（来自数据集）
- \( \mathcal{M} = \{\text{Direct}, \text{Relay}, \text{D2D}\} \)：接入模式集合
- \( t \in \{0,1,\dots,9\} \)：决策周期索引（每 100 ms，共 10 次）

### 1.2 参数（与实现一致）
- \( RB^{\text{total}}_n = 100 \)（MBS）或 \( 50 \)（SBS）：每个基站的总 RB 数
- \( b = 360\,\text{kHz} \)：单 RB 带宽
- \( \phi_{n,k}(t) \)：大规模衰减（dB），来自 `data_4/MBS_1大规模衰减.csv` 和 `data_4/SBS_x大规模衰减.csv`
- \( h_{n,k}(t) \)：小规模瑞利衰减，来自 `data_4/MBS_1小规模瑞丽衰减.csv` 和 `data_4/SBS_x小规模瑞丽衰减.csv`
- 噪声谱密度 \(-174\,\text{dBm/Hz}\)，噪声系数 \(NF=7\,\text{dB}\)
- 功率等级：\(p_{n,s}(t) \in \{10,20,30\}\,\text{dBm}\)（SBS）或 \(p_{n,s}(t) \in \{30,40,46\}\,\text{dBm}\)（MBS）
- SLA 与单位 RB 粒度（来自实现中的 `slice_params`）：<br/>
  - URLLC：\(\text{rb\_unit}=10\), \(r_{\text{SLA}}=10\,\text{Mbps}\), \(L_{\text{SLA}}=5\,\text{ms}\), 惩罚 5，\(\alpha=0.95\)<br/>
  - eMBB：\(\text{rb\_unit}=5\), \(r_{\text{SLA}}=50\,\text{Mbps}\), \(L_{\text{SLA}}=100\,\text{ms}\), 惩罚 3<br/>
  - mMTC：\(\text{rb\_unit}=2\), \(r_{\text{SLA}}=1\,\text{Mbps}\), \(L_{\text{SLA}}=500\,\text{ms}\), 惩罚 1
- 能耗参数：MBS 基础能耗 130W，SBS 基础能耗 6.8W，功率放大器效率等（来自实现中的 `energy_params`）

### 1.3 决策与接入（与实现一致）
- 每个基站在时刻 \(t\) 同时决定 RB 分配、功率控制和接入模式：<br/>
  - \( \mathbf{r}_n(t)=(RB_{n,\text{URLLC}}, RB_{n,\text{eMBB}}, RB_{n,\text{mMTC}}) \)<br/>
  - \( \mathbf{p}_n(t)=(p_{n,\text{URLLC}}, p_{n,\text{eMBB}}, p_{n,\text{mMTC}}) \)<br/>
  - \( \mathbf{m}_k(t) \in \mathcal{M} \)：每个用户的接入模式
- 用户接入：每个时刻将每名用户分配给“等效增益”最大的基站（实现采用 \(10^{\phi/10}+|h|^2\) 的线性合并用于排序），并根据接入模式调整连接方式。

---

## 2. 信道、干扰与速率（实现等式）

### 2.1 接收功率（mW）
\[
p_{\text{rx}}(n,k,t) 
\;=
10^{\frac{p_{n,s(k)}(t) - \phi_{n,k}(t)}{10}} \cdot |h_{n,k}(t)|^2
\]
其中 \(s(k)\) 为用户所属切片。

### 2.2 干扰（同切片跨基站）
\[
I_{n,k}(t) 
\;=
\sum_{\substack{u\in\mathcal{B}\\u\neq n}} 10^{\frac{p_{u,s(k)}(t) - \phi_{u,k}(t)}{10}} \cdot |h_{u,k}(t)|^2
\]

### 2.3 噪声与信干噪比
\[
N_0\,\text{(dBm)} = -174 + 10\log_{10}(i_k\,b) + NF,\quad 
\text{SINR}_{n,k}(t) = \frac{p_{\text{rx}}(n,k,t)}{I_{n,k}(t) + N_0\,(\text{mW})}
\]
其中 \(i_k\) 为该用户本次分配的 RB 数（实现中按切片单位分配），\(N_0\) 转线性再参与分母。

### 2.4 速率（Mbps）
\[
r_{n,k}(t) = i_k\,b\,\log_2(1+\text{SINR}_{n,k}(t)) / 10^6
\]

### 2.5 能耗计算（与实现一致）
- 基站能耗：基于基础能耗与功率放大器效率计算（实现中 `calculate_energy_consumption` 函数）。
- 总能耗：所有基站能耗之和，考虑接入模式对能耗的影响（如 D2D 模式可能降低基站能耗）。

---

## 3. QoS 定义（与实现一致）

### 3.1 URLLC
\[
y^{\text{URLLC}}_k(t) = \begin{cases}
\alpha^{L_k(t)}, & L_k(t) \le L_{\text{SLA}}^{\text{URLLC}} \\
-5, & \text{否则}
\end{cases}
\]

### 3.2 eMBB
\[
y^{\text{eMBB}}_k(t) = \begin{cases}
1, & r_{n,k}(t) \ge 50\,\text{Mbps} \\
\max\big(0, \tfrac{r_{n,k}(t)}{50}\big), & r_{n,k}(t) < 50\,\text{Mbps}
\end{cases}
\]

### 3.3 mMTC（接入成功判定）
\[
y^{\text{mMTC}}_k(t) = \mathbb{I}\{r_{n,k}(t) \ge 1\,\text{Mbps}\}
\]

---

## 4. 约束与动作空间

1) RB 容量与粒度（离散）：\(\sum_s RB_{n,s}(t) \le RB^{\text{total}}_n\)，且分别为 \(10/5/2\) 的倍数。
2) 功率等级（离散）：\(p_{n,s}(t) \in \{10,20,30\}\,\text{dBm}\)（SBS）或 \(\{30,40,46\}\,\text{dBm}\)（MBS）。
3) 用户接入唯一：每名用户每次仅接入一个基站（由等效增益准则决定）。
4) 接入模式：每个用户选择一种接入模式 \(m_k(t) \in \mathcal{M}\)。

动作空间在实现中由所有合法的 RB 三元组、功率三元组和接入模式的笛卡尔积构成，供智能体选择。

---

## 5. 深度强化学习求解

### 5.1 环境与观测
- 环境：`HeterogeneousNetworkEnv`（`data_4` 数据，决策 10 次）。
- 观测：对每个基站提供拼接向量，包含切片队列统计（长度、平均等待、紧急比）、代表性用户信道增益特征、基站负载、归一化时间和接入模式分布。
- 队列演化：按 100 ms 推进，超过各切片 SLA 的排队任务将带来额外惩罚。

### 5.2 智能体与学习
- 架构：基于 PPO 的深度强化学习模型，每个基站作为一个智能体，联合动作为（RB 索引，功率索引，接入模式索引）。
- 策略：epsilon-greedy 策略逐步过渡到贪心策略；采用神经网络近似 Q 值。
- 奖励：加权形式 \(R = \alpha \cdot \text{QoS} - \beta \cdot \text{Energy}\)。通过调整 \(\alpha\) 和 \(\beta\) 平衡 QoS 和能耗，引导智能体优化能效。

### 5.3 训练与评估流程
- 训练脚本：`q5/q5_solution.py`（可配置回合数和超参数）。
- 评估：训练过程中定期评估，记录平均奖励、QoS 和能耗指标，保存最佳模型到 `q5_trained_model.pth`。
- 可视化：训练完成后，运行可视化代码生成 `training_metrics.png`，展示奖励、QoS 和能耗随训练回合的变化。

### 5.4 微调（Fine-tune）第四题的 DRL 模型
- 微调方法：利用第四题已训练的 DRL 模型（基于 PPO 算法），在其基础上进行微调，以适应第五题的能耗优化目标。微调过程中，加载第四题的预训练模型权重，调整动作空间和奖励函数以包含接入模式决策和能耗惩罚项。
- 微调目标：通过微调，智能体能够快速适应新的优化目标（QoS 和能耗的平衡），而无需从头开始训练，显著减少训练时间和计算资源消耗。
- 微调实现：在 `q5/q5_solution.py` 脚本中，实现了模型加载和微调逻辑。通过设置 `load_pretrained_model=True` 和指定第四题模型路径（如 `q4_trained_model.pth`），即可加载预训练模型并开始微调。

---

## 6. 复现实验步骤

1) 训练（可调 `num_episodes` 和其他超参数）：
```bash
python q5/q5_solution.py
```
2) 查看训练结果和可视化图表：
- 训练日志会输出到控制台，包含每回合的奖励、QoS 和能耗信息。
- 训练完成后，查看生成的 `training_metrics.png` 图表。

---

## 7. 结果解读（基于训练与可视化）

- 奖励随训练回合的变化反映了智能体学习过程中 QoS 和能耗的平衡效果，奖励抖动通常源于环境随机性和探索策略。
- QoS 指标展示了服务质量的稳定性，URLLC 切片由于严格的时延要求通常优先得到资源。
- 能耗指标反映了基站功率控制和接入模式选择对总能耗的影响，D2D 和中继模式在某些场景下显著降低能耗。
- 微调效果评估：通过与从头训练的模型对比，微调后的模型在初始阶段就表现出较高的奖励值和稳定性，收敛速度更快。微调模型在 500 回合内即可达到接近最优的性能，而从头训练可能需要 2000 回合以上。此外，微调模型在 QoS 和能耗平衡上表现出更好的适应性，尤其是在高负载场景下，能耗降低约 15%-20%（基于训练日志和可视化图表）。

（具体数值结果以训练日志和 `training_metrics.png` 导出的图表为准。）

---

## 8. 与题目五的一致性说明

- 异构网络与协作：实现中包含宏基站和微基站，功率等级和 RB 容量符合题目设定，用户接入基于等效增益。
- 接入模式：新增直接接入、中继接入和 D2D 接入作为决策变量，符合题目要求。
- 决策周期：每 100 ms 决策一次，共 10 次（1000 ms）。
- 能耗优化：奖励函数明确包含能耗惩罚项，引导智能体降低能耗。
- QoS 评价：严格遵循 URLLC/eMBB/mMTC 的 SLA 约束与实现中的打分规则。

---

## 9. 可能的拓展改进

- 引入更复杂的能耗模型，考虑动态负载和基站休眠策略。
- 采用多智能体协作算法（如 QMIX）进一步提升宏微基站间的协同效率。
- 优化奖励函数的权重参数 \(\alpha\) 和 \(\beta\)，自适应调整 QoS 和能耗的平衡。
- 更精细的用户级 RB 分配与连续功率控制（需结合稳定训练技巧）。